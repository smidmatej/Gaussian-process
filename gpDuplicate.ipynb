{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4372fe36-e95a-4007-8005-627169db408b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from gp import *\n",
    "\n",
    "from numpy.random import default_rng\n",
    "from six.moves import cPickle as pickle #for performance\n",
    "rng = default_rng()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c20776b-e6ef-421c-bbb3-a726ef992fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dict(filename_):\n",
    "    with open(filename_, 'rb') as f:\n",
    "        ret_di = pickle.load(f)\n",
    "    return ret_di"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32419f0a-36f8-4191-a368-eeb1f04e59e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_acceleration_errors(data):\n",
    "    v = data['v'][1:]\n",
    "    v_pred = data['v_pred'][1:]\n",
    "    a_error = (v - v_pred)/data['dt']\n",
    "    a_validation = data['aero_drag'][1:]\n",
    "    \n",
    "    return a_error, a_validation\n",
    "\n",
    "def subsample_for_training(z, y, n):\n",
    "    n_sub = int(y.shape[0]/n)\n",
    "    \n",
    "    print(f'Number of subsamples = {n}')\n",
    "    print(f'Size of feature training data before subsampling = {z.shape}')\n",
    "    print(f'Size of output training data before subsampling = {y.shape}')\n",
    "\n",
    "    \n",
    "    z_train = z[::n_sub,:]\n",
    "    y_train = y[::n_sub,:]\n",
    "    \n",
    "    print(f'Size of feature training data after subsampling = {z_train.shape}')\n",
    "    print(f'Size of output training data after subsampling = {y_train.shape}')\n",
    "\n",
    "\n",
    "    return z_train, y_train\n",
    "\n",
    "def load_data():\n",
    "    data = load_dict('data/trajectory.pkl')\n",
    "    p = data['p'][1:,:]\n",
    "    q = data['q'][1:,:]\n",
    "    v = data['v'][1:,:]\n",
    "    w = data['w'][1:,:]\n",
    "    u = data['u'][1:,:]\n",
    "    \n",
    "    #print(data)\n",
    "    return p,q,v,w,u\n",
    "\n",
    "def subsample(z, reduction_factor):\n",
    "    print(f'Subsampling by factor of = {reduction_factor}')\n",
    "\n",
    "    z = z[::reduction_factor]\n",
    "    return z\n",
    "\n",
    "def shuffle_dataset(z, y):\n",
    "\n",
    "    dataset = np.concatenate((z,y), axis=1) # concatenates along the state axis\n",
    "\n",
    "    rng.shuffle(dataset)\n",
    "    \n",
    "    \n",
    "    z = dataset[:,:-3]\n",
    "    y = dataset[:,-3:]\n",
    "    return z,y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b8246e-553a-4e54-8904-53c7077eaa70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "053948f7-6d36-49fa-9c63-786d32a6bb2b",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/trajectory.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m p,q,v,w,u \u001b[38;5;241m=\u001b[39m \u001b[43mload_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# state vector\u001b[39;00m\n\u001b[1;32m      4\u001b[0m z \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate((p, q, v, w, u),axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[0;32mIn [3], line 27\u001b[0m, in \u001b[0;36mload_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_data\u001b[39m():\n\u001b[0;32m---> 27\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mload_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata/trajectory.pkl\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m     p \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mp\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m1\u001b[39m:,:]\n\u001b[1;32m     29\u001b[0m     q \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mq\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m1\u001b[39m:,:]\n",
      "Cell \u001b[0;32mIn [2], line 2\u001b[0m, in \u001b[0;36mload_dict\u001b[0;34m(filename_)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_dict\u001b[39m(filename_):\n\u001b[0;32m----> 2\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfilename_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      3\u001b[0m         ret_di \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ret_di\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/trajectory.pkl'"
     ]
    }
   ],
   "source": [
    "p,q,v,w,u = load_data()\n",
    "\n",
    "# state vector\n",
    "z = np.concatenate((p, q, v, w, u),axis=1)\n",
    "\n",
    "# dependent variable\n",
    "a_error, a_validation = calculate_acceleration_errors(load_dict('data/trajectory.pkl'))\n",
    "y = a_error \n",
    "\n",
    "# take a subset of data to reduce computation time during debugging\n",
    "reduction_factor = 100\n",
    "\n",
    "# shuffles the data so that we can take a equidistant subsampling without loss of information\n",
    "z,y = shuffle_dataset(z,y)\n",
    "\n",
    "# takes every *reduction_factor* index along first axis \n",
    "z = subsample(z, reduction_factor)\n",
    "y = subsample(y, reduction_factor)\n",
    "\n",
    "# subsample further into *n* samples for training\n",
    "z_train, y_train = subsample_for_training(z,y, 50)\n",
    "\n",
    "# divide y into 3 seperate arrays for xyz dimensions\n",
    "y_x, y_y, y_z = np.split(y_train, 3, axis = 1)\n",
    "                         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135953d5-9100-44e4-a36c-e1dc5bb525a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b55d17a-63b2-469d-94a7-5a4a6e4cabdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 10), dpi=100)\n",
    "plt.subplot(211)\n",
    "plt.plot(v[:,0], a_validation[:,0], 'r', markerfacecolor='none')\n",
    "plt.plot(v[:,1], a_validation[:,1], 'g', markerfacecolor='none')\n",
    "plt.plot(v[:,2], a_validation[:,2], 'b', markerfacecolor='none')\n",
    "plt.plot(v[:,0], a_error[:,0], 'r--')\n",
    "plt.plot(v[:,1], a_error[:,1], 'g--')\n",
    "plt.plot(v[:,2], a_error[:,2], 'b--')\n",
    "plt.xlabel('Velocity')\n",
    "plt.ylabel('Drag acceleration oracle')\n",
    "plt.title('Drag versus velocity')\n",
    "plt.legend(('Oracle drag','','', 'Estimated drag','',''))\n",
    "\n",
    "plt.subplot(212)\n",
    "plt.plot(a_error[:,0], a_validation[:,0], 'r')\n",
    "plt.plot(a_error[:,1], a_validation[:,1], 'g')\n",
    "plt.plot(a_error[:,2], a_validation[:,2], 'b')\n",
    "plt.xlabel('Estimated drag')\n",
    "plt.ylabel('Drag acceleration oracle')\n",
    "plt.title('Estimation is sufficient to approximate drag')\n",
    "plt.legend(('x', 'y', 'z'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844bea06-f3e3-4ca2-97f4-efb7665d2f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6), dpi=100)\n",
    "\n",
    "plt.plot(v[:,0], a_validation[:,0], '-')\n",
    "plt.plot(v[:,0], a_error[:,0], '--')\n",
    "plt.plot(z_train[:,7], y_x, '+')\n",
    "plt.xlabel('Velocity oracle')\n",
    "plt.ylabel('Drag acceleration')\n",
    "plt.title('Drag versus velocity')\n",
    "plt.legend(('Oracle drag velocity', 'Computed difference in acceleration'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5591e5b2-8b3a-4ba8-981e-ce1a7aa2a192",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kernel = RBF(L=np.eye(1)*theta_star[0], sigma_f=theta[1])\n",
    "# model = GPR(z_train, y_train, noise=theta_star[-1], covariance_function=kernel)\n",
    "\n",
    "\n",
    "# x_query = np.arange(-10,10,0.1).reshape(-1,1)\n",
    "# y_query = model.predict(x_query)\n",
    "\n",
    "# mean, var = model.predict(z, var=True)\n",
    "# print(mean.shape)\n",
    "# rms = np.sqrt(np.mean((y - mean)**2))\n",
    "# print(rms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ae5574-2508-4146-a1d1-2d608f3e6f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ae5022-2202-4dcb-b9a3-09a6cf701c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll(z_train, y_train, kernel):\n",
    "    def nll_stable(theta):\n",
    "\n",
    "        # Numerically more stable implementation of Eq. (11) as described\n",
    "        # in http://www.gaussianprocess.org/gpml/chapters/RW2.pdf, Section\n",
    "        # 2.2, Algorithm 2.1.\n",
    "        k = kernel(L=np.diag(theta[:-2]), sigma_f=theta[-2])\n",
    "    \n",
    "        K = calculate_covariance_matrix(z_train, z_train, k) + (theta[-1]+1e-7)*np.identity(z_train.shape[0])\n",
    "        L = cholesky(K)\n",
    "\n",
    "        S1 = solve_triangular(L, y_train, lower=True)\n",
    "        S2 = solve_triangular(L.T, S1, lower=False)\n",
    "\n",
    "        return (np.sum(np.log(np.diagonal(L))) + \\\n",
    "               0.5 * y_train.T.dot(S2) + \\\n",
    "               0.5 * z_train.shape[0] * np.log(2*np.pi)).flatten()\n",
    "    \n",
    "    \n",
    "    return nll_stable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116436d1-d5d1-42f7-a094-2f4b6b8c3c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "theta0 = [1,1,1]\n",
    "\n",
    "z_velx = z_train[:,7].reshape(-1,1) # just x velocity\n",
    "\n",
    "kernel = RBF(L=np.eye(z_velx.shape[1])*theta0[0], sigma_f=theta0[1])\n",
    "model = GPR(z_velx, y_x, noise=theta0[2], covariance_function=kernel)\n",
    "\n",
    "## TODO ADD scipy.minimize as in http://krasserm.github.io/2018/03/19/gaussian-processes/\n",
    "# also: http://gaussianprocess.org/gpml/chapters/RW2.pdf page 11\n",
    "\n",
    "x_query = np.arange(-10,10,0.1).reshape(-1,1)\n",
    "#print(x_query)\n",
    "mean, std = model.predict(x_query, std=True)\n",
    "\n",
    "print(mean.shape)\n",
    "print(std.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f6b092-222c-4498-9593-c58071c64610",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6), dpi=100)\n",
    "plt.plot(x_query, mean)\n",
    "plt.plot(z[:,7], y[:,0], '+')\n",
    "plt.plot(z_train[:,7], y_x[:,0], 'x')\n",
    "plt.fill_between(x_query.reshape(-1), mean.reshape(-1) - std, mean.reshape(-1) + std,\n",
    "                 color='gray', alpha=0.2)\n",
    "\n",
    "plt.xlabel('Velocity in x [ms-1]')\n",
    "plt.ylabel('Acceleration error in x [ms-2]')\n",
    "plt.title('GP fit')\n",
    "plt.legend(('GP fit', 'Measured samples', 'Training samples'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402d2659-a61e-4f11-9721-7348274d62b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# neg_log_fcn = nll(z_velx, y_x, RBF)\n",
    "\n",
    "# l_possible = np.linspace(0.1,10,10)\n",
    "# sigma_possible = np.linspace(1,5,10)\n",
    "# likelyhood = np.empty((l_possible.shape[0], sigma_possible.shape[0]))\n",
    "# for i in range(l_possible.shape[0]):\n",
    "#     for j in range(sigma_possible.shape[0]):\n",
    "#         likelyhood[i,j] = neg_log_fcn(np.array([l_possible[i], sigma_possible[j], 0.1]))\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14f70fa-ea75-45d5-a4ae-f54570fb88ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X, Y = np.meshgrid(sigma_possible, l_possible)\n",
    "# fig = plt.figure(figsize=(10,6), dpi=100)\n",
    "# ax = plt.axes(projection='3d')\n",
    "# ax.contour3D(X, Y, likelyhood, 50)\n",
    "# ax.set_xlabel('sigma')\n",
    "# ax.set_ylabel('length scale')\n",
    "# ax.set_zlabel('z');\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267b535f-1431-4b92-ba8c-63bebbb31b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "neg_log_fcn = nll(z_velx, y_x, RBF)\n",
    "\n",
    "bnds = ((0, None), (0, None), (0, None))\n",
    "sol_min = minimize(neg_log_fcn, x0=theta0, method='L-BFGS-B', bounds=bnds)\n",
    "theta_star = sol_min.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602fdbcb-cfc3-4ada-a49d-001fccb9d482",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_velx = z_train[:,7].reshape(-1,1) # just x velocity\n",
    "\n",
    "kernel = RBF(L=np.eye(z_velx.shape[1])*theta_star[0], sigma_f=theta_star[1])\n",
    "model = GPR(z_velx, y_x, noise=theta_star[2], covariance_function=kernel)\n",
    "\n",
    "## TODO ADD scipy.minimize as in http://krasserm.github.io/2018/03/19/gaussian-processes/\n",
    "# also: http://gaussianprocess.org/gpml/chapters/RW2.pdf page 11\n",
    "\n",
    "x_query = np.arange(-10,10,0.1).reshape(-1,1)\n",
    "#print(x_query)\n",
    "mean, std = model.predict(x_query, std=True)\n",
    "\n",
    "print(mean.shape)\n",
    "print(std.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f71173f-44f7-4c90-9bb6-1ef4d8d0af3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6), dpi=100)\n",
    "plt.plot(x_query, mean)\n",
    "plt.plot(z[:,7], y[:,0], '+')\n",
    "plt.plot(z_train[:,7], y_x[:,0], 'x')\n",
    "plt.fill_between(x_query.reshape(-1), mean.reshape(-1) - std, mean.reshape(-1) + std,\n",
    "                 color='gray', alpha=0.2)\n",
    "\n",
    "plt.xlabel('Velocity in x [ms-1]')\n",
    "plt.ylabel('Acceleration error in x [ms-2]')\n",
    "plt.title('GP fit')\n",
    "plt.legend(('GP fit', 'Measured samples', 'Training samples'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b081215-647c-41b9-b378-de58e1557f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(theta_star)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574738ff-f6b5-4b6b-b526-010419e266f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = RBF(L=np.eye(1))\n",
    "\n",
    "model = GPR(z_train[:,8].reshape(1,-1), y_train, noise=0.1, covariance_function=kernel)\n",
    "\n",
    "\n",
    "x_query = np.arange(-10,10,0.1).reshape(-1,1)\n",
    "y_query = model.predict(x_query)\n",
    "\n",
    "#mean, var = model.predict(v[:,0], var=True)\n",
    "print(mean.shape)\n",
    "rms = np.sqrt(np.mean((y - mean)**2))\n",
    "print(rms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682ded83-bea8-492d-9682-5285b60aee93",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(z[:,8], mean,'+')\n",
    "\n",
    "plt.plot(v[:,0], a_validation[:,0], 'r+')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9c9a0d-2966-410e-8484-2633a409ae6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# kernel = RBF(L=np.eye(z_vel.shape[1])*10)\n",
    "# model = GPR(v.reshape(-1,1), a_error.reshape(-1,1), noise=0.1, covariance_function=kernel)\n",
    "\n",
    "# x_query = np.arange(-10,10,0.1).reshape(-1,1)\n",
    "# print(x_query)\n",
    "# mean = model.predict(x_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffcb2109-b160-4739-a246-0140b8fb4e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(z_vel, y_train,'+')\n",
    "plt.plot(x_query, mean)\n",
    "plt.plot(v[:,0], a_error[:,0], 'r+')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e321f0-b016-4c01-abc7-ef9c103b28aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampling_ns = [3, 5, 10, 20, 50, 100]\n",
    "\n",
    "# rms = np.empty((len(sampling_ns),))*np.NaN\n",
    "\n",
    "# for i in range(len(sampling_ns)):\n",
    "#     n_samples = sampling_ns[i]\n",
    "#     n_sub = int(z_train.shape[0]/n_samples)\n",
    "#     z_subsampled = z_train[::n_sub]\n",
    "#     y_subsampled = y_train[::n_sub]\n",
    "#     y_subsampled = y_subsampled.reshape((-1,1))\n",
    "    \n",
    "#     print(y_subsampled.shape)\n",
    "#     kernel = SquaredExponentialKernel(L=np.eye(z_subsampled.shape[1]))\n",
    "\n",
    "#     model = GPR(z_subsampled, y_subsampled, noise=0.0, covariance_function=kernel)\n",
    "#     mean, var = model.predict(z_train, var=True)\n",
    "\n",
    "#     rms[i] = np.sqrt(np.mean((y_train - mean)**2))\n",
    "\n",
    "# rms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd53827e-6697-4d5a-b59a-2319d13aa9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(sampling_ns, rms)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
